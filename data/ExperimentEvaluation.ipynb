{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib qt\n",
    "\n",
    "from data import result_loader, plotter, plotter_evaluation, plotter_layer\n",
    "from data.plotter import PlotType as PT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment evaluation\n",
    "This interactive notebook loads specs, histories and models from files generated by a past IMP- or OSP-experiment and plots the results.\n",
    "\n",
    "The next cell prints all specs-files from the subdirectory `/results` and asks for the experiment to load.\n",
    "Simply copy a complete line into the text field and press enter to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "relative_paths = result_loader.get_relative_spec_file_paths()\n",
    "sep = '\\n'\n",
    "message = f\"{sep.join(relative_paths)}\\n\\nPlease choose a specs-file: \"\n",
    "specs_file = input(message)\n",
    "experiment_prefix = result_loader.extract_experiment_path_prefix(specs_file)\n",
    "absolute_specs_path = result_loader.generate_absolute_specs_path(specs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment's specs and results\n",
    "specs = result_loader.get_specs_from_file(absolute_specs_path, as_dict=False)\n",
    "hists = result_loader.get_experiment_histories_from_file(experiment_prefix)\n",
    "nets = result_loader.get_models_from_files(experiment_prefix, specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "plotter.plot_average_hists_on_ax(ax[0], hists.val_acc[:,::2,:120], hists.sparsity[::2], specs.plot_step, PT.VAL_ACC)\n",
    "plotter.plot_average_hists_on_ax(ax[1], hists.train_loss[:,::2,:120], hists.sparsity[::2], specs.plot_step, PT.TRAIN_LOSS, force_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "plotter.plot_average_hists_on_ax(ax[0], hists.train_loss, hists.sparsity, specs.plot_step, PT.TRAIN_LOSS, force_zero=True)\n",
    "plotter.plot_average_hists_on_ax(ax[1], hists.val_loss, hists.sparsity, specs.plot_step, PT.VAL_LOSS, force_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plotter.plot_acc_at_early_stop_on_ax(ax, hists.val_loss, hists.test_acc, hists.sparsity, specs.net, PT.TEST_ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plotter.plot_early_stop_iterations_on_ax(ax, hists.val_loss, hists.sparsity, specs.plot_step, specs.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plotter.plot_average_hists_on_ax(ax, hists.test_acc[:,::2,:120], hists.sparsity[::2], specs.plot_step, PT.TEST_ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare final test-accuracies\n",
    "for n in range(specs.net_count):\n",
    "    unpruned_final = hists.test_acc[n, 0, -1]\n",
    "    for p in range(1, specs.prune_count + 1):\n",
    "        pruned_final = hists.test_acc[n, p, -1]\n",
    "        print(f\"Test-accuracy net #{n} (sparsity: {hists.sparsity[p]:.4}): {pruned_final:1.4} (changed by {(pruned_final-unpruned_final):1.4})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare final validation-accuracies\n",
    "for n in range(specs.net_count):\n",
    "    unpruned_final = hists.val_acc[n, 0, -1]\n",
    "    for p in range(1, specs.prune_count + 1):\n",
    "        pruned_final = hists.val_acc[n, p, -1]\n",
    "        print(f\"Validation-accuracy net #{n} (sparsity: {hists.sparsity[p]:.4}): {pruned_final:1.4} (changed by {(pruned_final-unpruned_final):1.4})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare test-accuracies at early-stop iteration\n",
    "early_stop_indices = plotter_evaluation.find_early_stop_indices(hists.val_loss)\n",
    "early_stop_acc = plotter_evaluation.get_values_at_stop_iteration(early_stop_indices, hists.test_acc)\n",
    "early_stop_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nets[0].sparsity_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with randomly reinitialized networks\n",
    "Once randomly reinitialized nets have been generated and trained, they can be plotted as dotted lines to compare winning tickets and arbitrary sparse nets.\n",
    "To retrain those random nets, it is necessary to generate `EarlyStopHistory` objects during the IMP-experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if specs.save_early_stop:\n",
    "    rhists = result_loader.get_all_random_experiment_histories_from_files(experiment_prefix, specs.net_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plotter.plot_average_hists_on_ax(ax, hists.test_acc[:,::,:120], hists.sparsity[::], specs.plot_step, PT.TEST_ACC, rhists.test_acc[:,::,:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plotter.plot_acc_at_early_stop_on_ax(ax, hists.val_loss, hists.test_acc, hists.sparsity, specs.net, PT.TEST_ACC, rhists.val_loss, rhists.test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plotter.plot_early_stop_iterations_on_ax(ax, hists.val_loss, hists.sparsity, specs.plot_step, specs.net, rhists.val_loss)\n",
    "\n",
    "# example to save a figure as PDF-file\n",
    "# fig.savefig(\"Early_stop_test.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of linear and convolutional layers\n",
    "The weights from linear layers and kernels from convolutional layers can be plotted.\n",
    "In this example all compatible layers from the first net are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_layer.plot_conv(nets[0].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_layer.plot_fc(nets[0].fc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
